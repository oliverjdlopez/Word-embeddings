













Refs:

1 (vocab_file)





not sure why I had to make the times global
embedding dim is hardcoded
max_vocab will terminate if not enough
-english words
-only alphas accepted

start and end tokens are dropped
- everything else is averaged together
layers are negatively indexed
tokenizer and model (huggingface)
find vocab file
sklearn for tsne, pca
pytorch for 